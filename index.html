<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="A mind needs books.">
<meta property="og:type" content="website">
<meta property="og:title" content="My Blog">
<meta property="og:url" content="http://leeliang.github.io/index.html">
<meta property="og:site_name" content="My Blog">
<meta property="og:description" content="A mind needs books.">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="My Blog">
<meta name="twitter:description" content="A mind needs books.">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://leeliang.github.io/"/>





  <title>My Blog</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-105614733-1', 'auto');
  ga('send', 'pageview');
</script>





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">My Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leeliang.github.io/2017/09/04/adaboost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang LI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/04/adaboost/" itemprop="url">机器学习算法(6): 集成学习之 AdaBoost</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-04T20:20:25+08:00">
                2017-09-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1-Boosting"><a href="#1-Boosting" class="headerlink" title="1.  Boosting"></a>1.  Boosting</h1><p>Boosting 是一族将弱学习器提升为强学习器的算法。工作机制类似：</p>
<ol>
<li>从初始训练集中训练出一个基学习器；</li>
<li>根据基学习器的表现调整样本的权值，使得先前基学习器做错的样本权值更大；</li>
<li>然后基于调整之后的权值训练下一个基学习器；</li>
<li>反复学习，得到一系列基学习器，组合这些基学习器。</li>
</ol>
<p>对于 Boosting 算法来说，关键点在于以下两个问题：</p>
<ol>
<li>如何改造权值？</li>
<li>如果组合基学习器？</li>
</ol>
<p>Boosting 族算法最著名的代表是 AdaBoost。</p>
<h1 id="2-AdaBoost-算法"><a href="#2-AdaBoost-算法" class="headerlink" title="2. AdaBoost 算法"></a>2. AdaBoost 算法</h1><p>简单来说，AdaBoost 算法就是根据指定参数 $M$，进行 $M$ 轮训练，得到 $M$ 个弱学习器 $G_i$ 及每个弱学习器的权重 $\alpha_i$，最终将这些弱学习器进行线性组合得到最终的学习器 $G$。</p>
<p>给定训练样本集 $\left \{ (x_i,y_i)\right \}_{i=1}^N$，其中 $x_i \in \mathbb{R}^n $ 为特征空间，$y_i \in \{ -1,+1 \}$ 为类标签。 AdaBoost 算法流程如下：</p>
<p>(1). 初始化训练集的权值分布：</p>
<p>\begin{equation}<br>D_{1}=(w_{11},…,w_{1i},…,w_{1N}),~w_{1i}=\frac{1}{N},~i=1,2,…,N<br>\end{equation}</p>
<p>(2) 对 $m=1,2,3…,M$:<br>(2.1) 使用权值分布 $D_m$ 的训练集学习，得到基学习器：<br>\begin{equation}<br>G_{m}(x):~X{\rightarrow}\{-1,+1\}<br>\end{equation}<br>(2.2) 计算 $G_{m}(x)$ 在训练集上的误差率：<br>\begin{equation}<br>e_{m}=P(G_{m}(x_{i})\neq y_{i}) =\sum_{i=1}^n w_{mi}I(G_{m}(x_{i})\neq y_{i})<br>\end{equation}<br>其中，</p>
<p>\begin{equation}<br>I(G_{m}(x_{i})\neq y_{i}) =<br>  \left\{<br>   \begin{array}{ccc}<br>           0,~ G_{m}(x_{i})&amp;=&amp; y_i \\<br>           1,~ G_{m}(x_{i})&amp;\neq&amp; y_i<br>   \end{array}<br>  \right.<br>\end{equation}<br>(2.3) 计算 $G_{m}(x)$ 的系数：<br>\begin{equation}<br>\alpha_{m}=\frac{1}{2}log \frac{1-e_{m}}{e_{m}}<br>\end{equation}<br>如图 1 所示，基学习器误差越大，其系数越小。<br><img src="/img/adaboost/ae.png" width="400" height="320" title="图1 基学习器系数随误差率的变化"><br>(2.4) 更新训练集的权值分布：<br>\begin{equation}<br>\begin{split}<br>&amp; D_{m+1}=(w_{m+1,1},…,w_{m+1,i},…,w_{m+1,N}) \\\<br>&amp; w_{m+1,i}= \left\{<br>   \begin{array}{ccc}<br>          \frac{w_{mi}}{Z_{m}}~ e^{-\alpha_i},~ G_{m}(x_{i})&amp;=&amp; y_i \\<br>          \frac{w_{mi}}{Z_{m}}~ e^{\alpha_i},~ G_{m}(x_{i})&amp;\neq&amp; y_i<br>   \end{array}<br>  \right.<br>\end{split}<br>\end{equation}<br>其中，<br>\begin{equation}<br>Z_{m} = \sum_{i=1}^{N}w_{mi}exp(-\alpha_{m}y_{i}G_{m}(x_{i}))<br>\end{equation}<br>为归一化因子。</p>
<p>（3） 构建线性分类器的线性组合：</p>
<p>\begin{equation}<br>f(x)=\sum_{m=1}^{M}\alpha_{m}G_{m}(x)<br>\end{equation}</p>
<p>得到最终分类器：<br>\begin{equation}<br>G(x)=sign(f(x))=sign(\sum_{m=1}^{M}\alpha_{m}G_{m}(x))<br>\end{equation}</p>
<h1 id="3-AdaBoost-的解释之损失函数优化"><a href="#3-AdaBoost-的解释之损失函数优化" class="headerlink" title="3. AdaBoost 的解释之损失函数优化"></a>3. AdaBoost 的解释之损失函数优化</h1><p>上一节提及到了学习器系数和权值更新公式，但是没有解释这个公式的来源，其实它们可以从从损失函数优化问题的角度来解读。</p>
<p>AdaBoost 还有另一个解释，即可以认为 Adaboost 是模型为加法模型，学习算法为前向分步学习算法，损失函数为指数函数的分类问题。</p>
<h2 id="3-1-加法模型"><a href="#3-1-加法模型" class="headerlink" title="3.1. 加法模型"></a>3.1. 加法模型</h2><p>加法模型 (additive model) 为：<br>\begin{equation}<br>f(x) = \sum_{m=1}^M \beta_m b(x; \gamma_m)<br>\end{equation}<br>其中，$b(x; \gamma_m)$ 为基函数，$\gamma_m$ 为基函数的参数，$\beta_m$ 为基函数的系数。</p>
<p>很明显，AdaBoost 的最终分类器等价于加法模型。现在的问题是该怎么样得到最终分类器，也就是其中的参数呢？</p>
<h2 id="3-2-指数损失函数优化"><a href="#3-2-指数损失函数优化" class="headerlink" title="3.2. 指数损失函数优化"></a>3.2. 指数损失函数优化</h2><p>AdaBoost 的损失函数是指数损失函数，即求得的最终分类器：<br>\begin{equation}<br>f(x)=\sum_{m=1}^{M}\alpha_{m}G_{m}(x)<br>\end{equation}<br>满足最小化指数损失函数：</p>
<p>\begin{equation}<br>L(y,f(x)) = e^{-yf(x)}<br>\end{equation}</p>
<p>通常这是一个复杂的优化问题。需要采用前向分布算法来求解。</p>
<h2 id="3-3-前向分布算法"><a href="#3-3-前向分布算法" class="headerlink" title="3.3 前向分布算法"></a>3.3 前向分布算法</h2><p>前向分布算法的思想是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近目标函数，那么就可以简化优化的复杂度。</p>
<p>AdaBoost 第一个基分类器是直接将基学习器算法用于初始训练集得到；此后利用前一个弱学习器的结果，迭代生成权值和基学习器系数，生成后一个弱学习器。第 $m$ 轮的学习器与第 $m-1$ 轮的学习器有如下关系：<br>\begin{equation}<br>f_{m}(x) =   f_{m-1}(x) + \alpha_m G_m(x)<br>\end{equation}<br>目标是每一轮的学习过程中得到 $\alpha_m$ 和 $G_m(x)$，使得 $f_m(x)$ 的指数损失最小，从而最终分类器的指数损失最小。</p>
<p>根据前向分布学习算法的关系，第 $m$ 轮得到的损失函数为：<br>\begin{equation}<br>\begin{split}<br>&amp;arg ~\min_{\alpha_m, G_m} e^{-y f_m{(x)}}\\\<br>&amp;=arg ~\min_{\alpha_m, G_m} e^{-y~(f_{m-1}(x)+\alpha_m G_m(x) )}\\\<br>&amp;=arg ~\min_{\alpha_m, G_m} \sum_{i=1}^N e^{-y_i~(f_{m-1}(x_i)+\alpha_m G_m(x_i) )}<br>\end{split}<br>\end{equation}<br>令 $w_{mi} = e^{-y_i~f_{m-1}(x_i)}$，它的值不依赖于 $\alpha_m$、$G_m(x)$，仅仅依赖于 $f_{m-1}$，随着每一轮迭代而改变。</p>
<p>先求得 $G^*_m(x)$，再上式对 $\alpha_m$ 求导，得到:<br>\begin{equation} \alpha^*_{m}=\frac{1}{2}log \frac{1-e_{m}}{e_{m}} \end{equation}<br>也就是上文的 (5) 式，即学习器系数。</p>
<p>根据 (13) 式和 $w_{mi} = e^{-y_i~f_{m-1}(x_i)}$，有：<br>\begin{equation} \begin{split}  w_{m+1,i}= \left\{    \begin{array}{ccc}           w_{mi}~ e^{-\alpha_i},~ G_{m}(x_{i})&amp;=&amp; y_i \\           w_{mi}~ e^{\alpha_i},~ G_{m}(x_{i})&amp;\neq&amp; y_i    \end{array}   \right. \end{split} \end{equation}<br>其与 (6) 式的权值更新公式只差归一化因子，因而等价。</p>
<p>以上，就是 AdaBoost 的另一种解释。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>[1] 《<a href="https://book.douban.com/subject/26708119/" target="_blank" rel="external">机器学习</a>》, 周志华</p>
<p>[2] 《<a href="https://book.douban.com/subject/10590856/" target="_blank" rel="external">统计学习方法</a>》, 李航</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leeliang.github.io/2017/09/01/random-forest/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang LI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/01/random-forest/" itemprop="url">机器学习算法(5): 集成学习之 随机森林</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-01T20:20:25+08:00">
                2017-09-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1-随机森林相关知识"><a href="#1-随机森林相关知识" class="headerlink" title="1. 随机森林相关知识"></a>1. 随机森林相关知识</h1><p>随机森林就是通过集成学习的思想将多棵决策树集成的一种算法，它的基本单元是决策树，而它的本质属于机器学习的集成学习方法。</p>
<h2 id="1-1-集成学习"><a href="#1-1-集成学习" class="headerlink" title="1.1. 集成学习"></a>1.1. 集成学习</h2><p>集成学习是指使用<strong>某种规则</strong>把多个学习器进行整合来完成学习任务的方法。</p>
<p>举个例子：<br>假设有三根决策树，它在三个测试样本上的表现如下，正确用 <code>对</code> 表示，错误用 <code>错</code> 表示：</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">测试样本 1</th>
<th style="text-align:center">测试样本 2</th>
<th style="text-align:center">测试样本 3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">决策树 1</td>
<td style="text-align:center">错</td>
<td style="text-align:center">对</td>
<td style="text-align:center">对</td>
</tr>
<tr>
<td style="text-align:center">决策树 2</td>
<td style="text-align:center">对</td>
<td style="text-align:center">错</td>
<td style="text-align:center">对</td>
</tr>
<tr>
<td style="text-align:center">决策树 3</td>
<td style="text-align:center">对</td>
<td style="text-align:center">对</td>
<td style="text-align:center">错</td>
</tr>
</tbody>
</table>
<p>采用<code>投票法</code>规则将三个决策树整合起来，投票法即少数服从多数。集成学习之后的表现为：</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">测试样本 1</th>
<th style="text-align:center">测试样本 2</th>
<th style="text-align:center">测试样本 3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">集成</td>
<td style="text-align:center">对</td>
<td style="text-align:center">对</td>
<td style="text-align:center">对</td>
</tr>
</tbody>
</table>
<p>上面就是集成学习的一个例子。</p>
<h2 id="1-2-Bagging"><a href="#1-2-Bagging" class="headerlink" title="1.2. Bagging"></a>1.2. Bagging</h2><p>为了完成集成学习，需先学习个体学习器，然后按照某种规则将个体学习器整合。对于随机森林来说，其个体学习器为<a href="/2017/08/21/decision-tree/" title="决策树">决策树</a>。</p>
<p>为了得到好的集成结果，需要个体学习器尽可能满足两个条件：</p>
<ul>
<li>个体学习器准确；</li>
<li>个体学习器之间相互独立。</li>
</ul>
<p>假设个体学习器为 10 个，对于给定的训练集，分为 10 个互斥的数据子集。根据数据子集训练出 10 个个体学习器，这样得到的个体学习器满足独立的条件。但是，由于数据子集的数据量占比 (10%) 较少，得到的个体学习器很可能不够准确。反之，如果数据子集中包含了训练集中的大量样本，每个个体学习器的训练数据高度相似，得到的个体学习器之间相关性很大。为了解决这个问题，需采样 Bagging 方法。</p>
<p>Bagging 是基于 bootstrap sampling。bootstrap 采样是有放回抽样，如果训练集的大小为 $N$，随机且有放回的从训练集中抽样 $N$ 次，得到一个新的训练集。重复上述过程 $T$ 次，可以得到 $T$ 个训练集，每个训练集的大小为 $N$。然后根据 $T$ 个训练集训练出 $T$ 个个体学习器，再将这些个体学习器整合起来，这就是 Bagging 的基本思想。</p>
<h1 id="2-随机森林"><a href="#2-随机森林" class="headerlink" title="2. 随机森林"></a>2. 随机森林</h1><p>随机森林是 Bagging 的扩展。除了 bootstarp sampling 抽样过程有随机外，在决策树的生成过程中也存在“随机”。</p>
<p>假设数据训练集的大小为 $N$，随机森林算法中，每颗决策树的生成过程如下：</p>
<ol>
<li>随机且有放回的从训练集中抽取 $N$ 个样本，形成一个新的训练集，作为该树的训练集；</li>
<li>传统决策树在最优特征选择时，是在当前结点的特征集中选择一个最优特征；而随机森林中，随机地从特征集中选取 $m$ 个特征，每次树进行分裂时，从这 $m$ 个特征中选取最优特征；</li>
<li>每颗树尽量生长且没有剪枝。</li>
</ol>
<p>通过上述方法生成组成随机森林的每颗决策树，然后采用某规则将这组决策树整合起来，在随机森林中，某规则一般指投票法。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>[1] 《<a href="https://book.douban.com/subject/26708119/" target="_blank" rel="external">机器学习</a>》, 周志华</p>
<p>[2] <a href="http://www.cnblogs.com/maybe2030/p/4585705.html" target="_blank" rel="external">随机森林（Random Forest）</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leeliang.github.io/2017/08/30/knn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang LI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/30/knn/" itemprop="url">机器学习算法(4): k 近邻</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-30T20:20:25+08:00">
                2017-08-30
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1-k-近邻概述"><a href="#1-k-近邻概述" class="headerlink" title="1.  k 近邻概述"></a>1.  k 近邻概述</h1><p>k 近邻法是一种常用的监督学习方法。给定一个训练集，其中样本的类别已定。分类时，基于某种距离度量找出训练集中与其最靠近的 k 个训练样本，然后根据这 k 个样本的类别通过多数表决等方式进行预测。k 近邻法不具有显式的训练过程，是懒惰学习 (lazy learning) 的著名代表。其中，关键的内容为距离度量和 k 值的选择。</p>
<h2 id="1-1-距离度量"><a href="#1-1-距离度量" class="headerlink" title="1.1 距离度量"></a>1.1 距离度量</h2><p>常用的距离度量有闵可夫斯基距离：</p>
<p>\begin{equation}<br>L_p(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{1/p}<br>\end{equation}</p>
<ol>
<li><p>$p=2$ 时，欧式距离：<br>\begin{equation}<br>L_p(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^2)^{1/2}<br>\end{equation}</p>
</li>
<li><p>$p=1$ 时，曼哈顿距离：<br>\begin{equation}<br>L_p(x_i,x_j)=\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|<br>\end{equation}</p>
</li>
<li><p>$p=\infty$ 时，切比雪夫距离：<br>\begin{equation}<br>L_p(x_i,x_j)=\max_l |x_i^{(l)}-x_j^{(l)}|<br>\end{equation}</p>
</li>
</ol>
<h2 id="1-2-k-值的选择"><a href="#1-2-k-值的选择" class="headerlink" title="1.2. k 值的选择"></a>1.2. k 值的选择</h2><p>k 值的选择会对结果产生重大影响。</p>
<p>k 值选择较小，则分类结果容易因为噪声点的干扰而出现错误。换句话说，k 值较小意味模型复杂，容易过拟合。</p>
<p>k 值选择较大，意味着与输入样本较远的训练样本也会对结果起作用，使得预测结果错误。</p>
<p>通常做法是利用交叉验证评估一系列不同的 k 值，选取结果最好的 k 值作为训练参数。</p>
<h1 id="2-kd-树"><a href="#2-kd-树" class="headerlink" title="2. kd 树"></a>2. kd 树</h1><p>现在我们有了 k 近邻的思想。对于一个输入样本，我们怎么样选取它的 k 个近邻呢？</p>
<p>最简单的方法就是线性扫描，计算输入样本和训练集中每一个样本的距离。但是当训练样本很大时，这种方法是不可取的。</p>
<p>为了提高 k 个近邻的搜索速度，可以采用特殊的结构存储训练数据，这里介绍 kd 树方法。</p>
<h2 id="2-1-kd-树的构造"><a href="#2-1-kd-树的构造" class="headerlink" title="2.1. kd 树的构造"></a>2.1. kd 树的构造</h2><p>kd 树是一种对 k 维空间样本点进行存储的树形数据结构，表示对 k 维空间的划分。构造 kd 树的过程相当于用垂直于某一维度的超平面将 k 维空间切分。</p>
<p>构造平衡 kd 树的过程如下：</p>
<ol>
<li>在训练集中选择具有最大方差的维度 k，然后在该维度上选择中位数为切分点，对该数据集合进行划分，得到两个子集合，划分的超平面为过切分点且垂直于 k 维度坐标轴的超平面；同时，将落在划分超平面上的样本点保存为根节点。</li>
<li>对两个子集重复 <code>1</code> 步骤，直至所有子集合不能再划分为止；如果某个子集合不能划分，则将该子集合中的数据保存到叶子节点。</li>
</ol>
<p>图 1 所示为 kd 树构造过程，数据来源于 wikipedia。首先在维度 $X^{(0)}$ 上选取切分点 <code>A</code>，分为两个子集合，并把 <code>A</code> 保存为根节点…………。<br><img src="/img/knn/kdtree.png" title="图1. 构造 kd 树过程，左边为特征空间的切分，右边为构造的 kd 树。"></p>
<h2 id="2-2-kd-树搜索"><a href="#2-2-kd-树搜索" class="headerlink" title="2.2. kd 树搜索"></a>2.2. kd 树搜索</h2><p>假设已经构造了 kd 树，我们有一个新的输入 $x$，搜索它的最近邻。<br>其算法如下：</p>
<ol>
<li>在 kd 树中找出包含目标点 $x$ 的叶结点：从根结点出发，递归的向下访问 kd 树。比较当前维上 $x$ 与切分点的坐标，若小移动到左子结点，否则右边，知道叶结点为止；</li>
<li>将到达的叶节点记为“当前最近点”，离目标点的距离为最近距离；</li>
<li><p>递归向上回溯，在每个结点作如下操作：</p>
<blockquote>
<p>3.1 如果该结点上的样本离目标点更远，向上回溯；<br>3.2 如果该结点上的样本距离目标点更近，将该结点标记为当前最近点，更新最近距离；并检查该结点的另一子结点对应的区域是否与以目标点为球心，最近距离为半径的超球体相交。若相交，移动到另一个子节点，向下落到叶节点，递归地进行最近邻搜索。若不相交，向上回溯。</p>
</blockquote>
</li>
<li><p>当回退到根节点时，搜索结束。 </p>
</li>
</ol>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>[1] 《<a href="https://book.douban.com/subject/26708119/" target="_blank" rel="external">机器学习</a>》, 周志华</p>
<p>[2] 《<a href="https://book.douban.com/subject/10590856/" target="_blank" rel="external">统计学习方法</a>》, 李航</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leeliang.github.io/2017/08/28/svm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang LI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/28/svm/" itemprop="url">机器学习算法(3): 支持向量机</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-28T20:20:25+08:00">
                2017-08-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1-支持向量机"><a href="#1-支持向量机" class="headerlink" title="1.  支持向量机"></a>1.  支持向量机</h1><p>给定训练样本集 $\left \{ (x_i,y_i)\right \}_{i=1}^N$，其中 $x_i \in \mathbb{R}^n $ 为特征空间，$y_i \in \{ -1,+1 \}$ 为类标签。支持向量机是基于划分超平面 (separating hyperplane) 将数据分类的分类器。如图 1 所示，二维数据的线性划分超平面为一条直线。<br><img src="/img/svm/svm1.png" title="图1. 可以划分数据的划分超平面"><br>可以看到，具有多个划分超平面可以将数据分类，我们需要找出唯一的划分超平面，使得划分效果最优，SVM 采用的是最大间隔 (maximum margin) 划分超平面。如图 2 所示。<br><img src="/img/svm/svm2.png" title="图2. 最大间隔划分超平面"><br>图 2 中的实心点，刚好处在划分超平面上，这些点确定了划分超平面，称之为支持向量。</p>
<p>上面的例子为线性分类器，即超平面可以用一个线性方程表示：<br>\begin{equation}<br>\mathbf{w}^T \cdot \mathbf{x} +b= 0<br>\end{equation}</p>
<p>对于需要非线性超平面才能分离的数据，SVM 采用核方法，这也是要引入对偶问题最大的原因。 </p>
<p>先基于线性可分支持向量机讲述原理，之后说明线性不可分支持向量机，<br>最后说明非线性支持向量机。</p>
<h1 id="2-线性可分支持向量机"><a href="#2-线性可分支持向量机" class="headerlink" title="2. 线性可分支持向量机"></a>2. 线性可分支持向量机</h1><h2 id="2-1-最大间隔超平面"><a href="#2-1-最大间隔超平面" class="headerlink" title="2.1. 最大间隔超平面"></a>2.1. 最大间隔超平面</h2><p>上面提到支持向量机的最优划分标准是最大间隔。在特征空间中，划分超平面可用如下线性方程表示:<br>\begin{equation}<br>\mathbf{w}^T \cdot \mathbf{x} +b= 0<br>\end{equation}<br>其中 $\mathbf{w}$ 为超平面的法向量，$b$ 为位移项。样本点 $x_i$ 到划分超平面的几何距离为：<br>\begin{equation}<br>\gamma_i = \frac{|\mathbf{w} \cdot x_i +b|}{||\mathbf{w}||}<br>\label{eqn3}<br>\end{equation}<br>$||\mathbf{w}||$ 为 $\mathbf{w}$ 的 $L_2$ 范数。<br>假设超平面能够将训练样本正确分类，即若 $\mathbf{w} \cdot x_i +b &gt;0$，则 $y_i = 1$，反之亦然。所以 (\ref{eqn3}) 式可以去掉绝对值，几何距离重写为：<br>\begin{equation}<br>\gamma_i = y_i\frac{\mathbf{w} \cdot x_i +b}{||\mathbf{w}||}<br>\end{equation}<br>我们定义训练集中最小的几何距离为<strong>几何间隔</strong>：<br>\begin{equation}<br>\gamma = \min \gamma_i<br>\end{equation}<br>最大间隔思想就是最大化几何间隔，在满足所有样本点的几何距离大于几何间隔的条件下：<br>\begin{equation}<br>\begin{split}<br>&amp;\max_{\mathbf{w},b}\gamma \\ &amp;s.t. \ \ y_i\frac{\mathbf{w} \cdot x_i +b}{||\mathbf{w}||} \ge \gamma, \ \ i = 1,2,…,N<br>\end{split}<br>\label{eqn6}<br>\end{equation}<br>为了简化问题，我们引入一个新的定义，<strong>函数间隔</strong>：<br>\begin{equation}<br>\widehat{\gamma} = \frac{\gamma }{||\mathbf{w}||}<br>\end{equation}<br>所以，(\ref{eqn6}) 式重写为：</p>
<p>\begin{equation}<br>\begin{aligned} &amp;\max_{\mathbf{w},b} \frac{\widehat{\gamma }}{||\mathbf{w}||} \\ &amp;s.t. \ \ y_i\left ( \mathbf{w}\cdot x_i + b \right ) \ge \widehat{\gamma }, \ \ i = 1,2,…,N \end{aligned}<br>\label{eqn8}<br>\end{equation}<br>可以看到，我们自由缩放函数间隔 $N$ 倍：<br>\begin{equation}<br>N ~\widehat{\gamma} = N~ y_i (\mathbf{w} \cdot x_i +b)<br>\end{equation}<br>超平面 $\mathbf{w}^T \cdot \mathbf{x} +b= 0$ 与 缩放后的平面 $N~(\mathbf{w}^T \cdot \mathbf{x} +b)= 0$ 相同；几何间隔同样也不会变化。也就是说，任意缩放函数间隔，对于我们最大化几何间隔问题的解算没有影响。为了简化我们的问题，令函数间隔 $\widehat{\gamma}=1$，这也是引入函数间隔的原因。(\ref{eqn8}) 式简化为：</p>
<p>\begin{equation}<br>\begin{aligned} &amp;\max_{\mathbf{w},b} \frac{ 1 }{||\mathbf{w}||} \\ &amp;s.t. \ \ y_i\left ( \mathbf{w}\cdot x_i + b \right ) \ge 1, \ \ i = 1,2,…,N \end{aligned}<br>\end{equation}</p>
<p>注意到最大化 $\frac{ 1 }{||\mathbf{w}||}$ 等价于最小化 $\frac{ 1 }{2}||\mathbf{w}||^2$，最终，SVM 需要解决的问题为以下有约束的最优化问题，更具体地，该问题为凸二次规划问题：<br>\begin{equation}<br>\begin{aligned} &amp;\min_{\mathbf{w},b} \frac{ 1 }{2}||\mathbf{w}||^2 \\ &amp;s.t. \ \ y_i\left ( \mathbf{w}\cdot x_i + b \right ) \ge 1, \ \ i = 1,2,…,N \end{aligned}<br>\label{eqn11}<br>\end{equation}</p>
<h2 id="2-2-对偶问题"><a href="#2-2-对偶问题" class="headerlink" title="2.2. 对偶问题"></a>2.2. 对偶问题</h2><p>凸二次规划问题可以使用现成的优化计算包计算，为了引入核函数和方便计算，将原问题转为其对偶问题进行求解。</p>
<h3 id="2-2-1-原始问题"><a href="#2-2-1-原始问题" class="headerlink" title="2.2.1. 原始问题"></a>2.2.1. 原始问题</h3><p>将原问题 (\ref{eqn11}) 式重写为：<br>\begin{equation}<br>\begin{aligned} &amp;\min_{\mathbf{w},b} \frac{ 1 }{2}||\mathbf{w}||^2 \\ &amp;s.t. \ \ g(\mathbf{w},b) \le 0 , \ \ i = 1,2,…,N \end{aligned}<br>\label{eqn12}<br>\end{equation}<br>其中 $g(\mathbf{w},b)=1- y_i\left ( \mathbf{w}\cdot x_i + b \right )$。<br>首先，定义拉格朗日函数为：</p>
<p>\begin{equation}<br>L(\mathbf{w},b,\alpha) = \frac{1}{2} ||\mathbf{w}||^2 - \sum_{i=1}^N \alpha_i ~g(\mathbf{w},b)<br>\end{equation}<br>其中，$\alpha_i \ge 0$。引入 $(\mathbf{w},b)$ 的函数，记为：</p>
<p>\begin{equation}<br>\theta_p(\mathbf{w},b) = \max_{\alpha} L(\mathbf{w},b,\alpha)<br>\end{equation}<br>若上式最大化问题有解，其必满足 (\ref{eqn12}) 式的条件，且其解为：</p>
<p>\begin{equation}<br> \frac{ 1 }{2}||\mathbf{w}||^2 = \max_{\alpha} L(\mathbf{w},b,\alpha)<br>\end{equation}<br>因为 $\alpha_i \ge 0$，需要 $g(\mathbf{w},b) \le 0$ 才能有最大值解。</p>
<p>所以，原问题等价于如下问题：</p>
<p>\begin{equation}<br>\min_{\mathbf{w},b} \max_{\alpha} L(\mathbf{w},b,\alpha)<br>\end{equation}</p>
<h3 id="2-2-2-原始问题的对偶问题"><a href="#2-2-2-原始问题的对偶问题" class="headerlink" title="2.2.2 原始问题的对偶问题"></a>2.2.2 原始问题的对偶问题</h3><p>记原始问题的对偶问题为：<br>\begin{equation}<br> \max_{\alpha} \min_{\mathbf{w},b}L(\mathbf{w},b,\alpha)<br>\end{equation}<br>记:<br> \begin{equation}<br> \theta_d(\alpha) = \min_{\mathbf{w},b} L(\mathbf{w},b,\alpha)<br>\end{equation}<br>我们要通过对偶问题的求解来解决原始问题，那么我们的问题是：对偶问题的解和原始问题的解有什么关系呢？</p>
<h3 id="2-2-3-原始问题与对偶问题的关系"><a href="#2-2-3-原始问题与对偶问题的关系" class="headerlink" title="2.2.3 原始问题与对偶问题的关系"></a>2.2.3 原始问题与对偶问题的关系</h3><p>对任意的 $(\mathbf{w},b,\alpha)$，有：</p>
<p>\begin{equation}<br>\theta_d(\alpha) = \min_{\mathbf{w},b} L(\mathbf{w},b,\alpha) \le L(\mathbf{w},b,\alpha) \le \max_{\alpha} L(\mathbf{w},b,\alpha) =  \theta_p(\mathbf{w},b)<br>\end{equation}</p>
<p>若原始问题和对偶问题都有解，根据上式，有：</p>
<p>\begin{equation}<br>\begin{split}<br>&amp;\theta_d(\alpha) \le \theta_p(\mathbf{w},b) \\\<br>&amp;\Rightarrow \max_{\alpha} \theta_d(\alpha) \le \min_{\mathbf{w},b}\theta_p(\mathbf{w},b) \\\<br>&amp;\Rightarrow \max_{\alpha} \min_{\mathbf{w},b}L(\mathbf{w},b,\alpha) \le \min_{\mathbf{w},b} \max_{\alpha} L(\mathbf{w},b,\alpha)<br>\end{split}<br>\end{equation}<br>即对偶问题的解小于或等于原始问题的解。在什么情况下，对偶问题的解与原始问题的解相同呢？如果能找到这个条件，那么我们直接解决对偶问题就可以得到原始问题的解，这个条件就是 KKT (Karush-Kuhn-Tucker) 条件。</p>
<p>KKT 条件为：</p>
<p>\begin{equation}<br>\begin{split}<br>\nabla_\mathbf{w} L(\mathbf{w}^*,b^*,\alpha^*) &amp;= 0 \\<br>\nabla_b L(\mathbf{w}^*,b^*,\alpha^*) &amp;= 0 \\<br>\nabla_\alpha L(\mathbf{w}^*,b^*,\alpha^*) &amp;= 0 \\<br>\alpha_i^* ~ g(\mathbf{w}^*,b^*) &amp;= 0, i=1,2,…,N \\<br>g(\mathbf{w}^*,b^*) &amp;\le 0, i=1,2,…,N \\<br>\alpha_i^* &amp;\ge 0, i=1,2,…,N<br>\end{split}<br>\end{equation}</p>
<p>如果 $(\mathbf{w}^*,b^*,\alpha^*)$ 满足 KKT 条件，则对偶问题和原始问题的解均为 $(\mathbf{w}^*,b^*,\alpha^*)$。</p>
<p>\begin{equation}<br>\begin{split}<br>&amp; \max_\alpha \min_{\mathbf{w},b}L(\mathbf{w},b,\alpha) \\<br>&amp;= \max_{\alpha} L(\mathbf{w}^*,b^*,\alpha) \\<br>&amp; = L(\mathbf{w}^*,b^*,\alpha^*) \\<br>&amp; = \frac{1}{2} ||\mathbf{w^*}||^2 - \sum_{i=1}^N \alpha_i^* ~g(\mathbf{w^*},b^*) \\<br>&amp; = \min_{\mathbf{w},b} \frac{ 1 }{2}||\mathbf{w}||^2 \  \ s.t. \ \ g(\mathbf{w^*},b^*) \le 0 , \ \ i = 1,2,…,N<br>\end{split}<br>\end{equation}</p>
<p>所以，SVM 问题转化为 KKT 条件下的对偶问题求解。</p>
<h3 id="2-2-3-对偶问题的求解"><a href="#2-2-3-对偶问题的求解" class="headerlink" title="2.2.3 对偶问题的求解"></a>2.2.3 对偶问题的求解</h3><p>为了得到对偶问题的解，需要先求 $L(\mathbf{w},b,\alpha)<br>$ 对 $(\mathbf{w},b)$ 的极小，再求对 $\alpha$ 的极大。</p>
<ol>
<li><p>求$\min \limits_{\mathbf{w},b}L(\mathbf{w},b,\alpha)$<br>将 $L(\mathbf{w},b,\alpha)$ 分别对 $\mathbf{w}$，$b$ 求偏导数并令其等于 $0$：<br>\begin{equation}<br>\begin{aligned}  \frac{\partial L(\mathbf{w},b,a)}{\partial \mathbf{w}} &amp;= 0 \Rightarrow \mathbf{w} - \sum_{i=1}^N a_iy_ix_i = 0 \\ \frac{\partial L(\mathbf{w},b,a)}{\partial b} &amp;= 0 \Rightarrow  - \sum_{i=1}^N a_i~y_i = 0 \end{aligned}<br>\end{equation}<br>将上式带入拉格朗日函数消去 $\mathbf{w}$，$b$，有：<br>\begin{equation}<br>\begin{aligned}<br>\min_{\mathbf{w},b}L(\mathbf{w},b,\alpha) =  \ &amp;-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Na_i a_jy_iy_j (x_i \cdot x_j) + \sum _{i=1}^Na_i \\<br>&amp; \ s.t. \ \ \ \sum_{i=1}^N a_iy_i = 0 \end{aligned}<br>\end{equation}</p>
</li>
<li><p>求对偶问题<br>\begin{equation}<br>\begin{aligned} &amp;\max_{a_i} \ -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Na_i a_jy_iy_j (x_i \cdot x_j) + \sum _{i=1}^Na_i\\ &amp; \ s.t. \ \ \ \sum_{i=1}^N a_iy_i = 0 \end{aligned}<br>\end{equation}</p>
</li>
</ol>
<p>根据上式，可以求取对偶问题的解 $\alpha^*$，再根据 KKT 的条件求取原始问题的解 $(\mathbf{w}^*,b^*)$，从而可以得到划分超平面，这种算法为对偶学习算法。</p>
<p>对偶问题的解是一个凸二次规划问题，理论上任何一个凸二次规划问题的软件包都可以解决，但是通常很慢，为了能够更快找到好的解，我们采用 <strong>SMO</strong> 算法。该算法的原理先放一放。</p>
<h1 id="3-线性不可分支持向量机"><a href="#3-线性不可分支持向量机" class="headerlink" title="3. 线性不可分支持向量机"></a>3. 线性不可分支持向量机</h1><p>上述的问题是基于所有样本点 $(x_i, y_i)$ 都能满足函数间隔大于等于 $1$ 的约束条件，即线性可分的。如果存在异常点不满足约束条件，上面的方法就不适用了。为了解决线性不可分的问题，可以对每个样本引入一个松弛变量 $\xi _i  \ge 0$，使得函数间隔加上松弛变量大于等于 $1$，即约束条件变为：<br>\begin{equation}<br>y_i(\mathbf{w} \cdot x_i +b) \ge 1 – \xi_i<br>\end{equation}<br>显然，$\xi_i$ 不能任意大，否则所有样本点都满足约束条件，为了约束 $\xi_i$ 的大小，需要在目标函数中加入惩罚。最终，优化目标改成如下的形式：</p>
<p>\begin{equation}<br>\begin{aligned} &amp;\min_{\mathbf{w},b,\xi}\ \ \frac{1}{2}||\mathbf{w}||^2 + C\sum_{i=1}^N \xi_i \\ &amp;s.t. \ \ \ \ -y_i(\mathbf{w} \cdot x_i + b) -\xi_i + 1\le 0, \ \ i = 1,2,…,N \\ &amp;\ \ \ \ \ \  \ \ \ \ -\xi_i \le 0 ,\ \ i = 1,2,…,N \end{aligned}<br>\end{equation}<br>其中，$C &gt; 0$ 称为惩罚参数，一般由应用问题决定，$C$ 值越大对误分类的惩罚越大。</p>
<p>同样地，线性不可分支持向量机的学习问题也是凸二次规划问题，我们采用与线性可分支持向量机同样的方法，即：</p>
<ol>
<li>构造拉格朗日函数；</li>
<li>原始问题转化为对偶问题；</li>
<li>利用 KKT 条件消去其他参数，得到只包含参数 $a_i$ 的对偶问题；</li>
<li>利用 <strong>SMO</strong> 算法解释出 $a_i$；</li>
<li>根据 KKT 条件和 $a_i$ 解算出 $\mathbf{w}~,b,$。</li>
</ol>
<p>这里给出对偶问题的具体形式：<br>\begin{equation}<br>\begin{aligned} &amp;\min_a \ \   \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N a_ia_jy_iy_j(x_i \cdot x_j) - \sum_{i=1}^Na_i \\ &amp;s.t. \ \ \ \ \  0 \le a_i \le C , \ i = 1,2,…,N\\ &amp;  \ \ \ \ \ \ \ \ \ \sum_{i=1}^Na_iy_i = 0,\  i = 1,2,…,N \end{aligned}<br>\end{equation}</p>
<p>可以看到，线性可分支持向量机可以认为是松弛变量等于$0$，也就是上式的特例。线性可分和线性不可分支持向量机合称为线性支持向量机。至此，上式就是线性支持向量机待学习的问题。</p>
<h1 id="4-线性支持向量机算法"><a href="#4-线性支持向量机算法" class="headerlink" title="4. 线性支持向量机算法"></a>4. 线性支持向量机算法</h1><hr>
<ol>
<li><p>构造约束最优化问题：<br>\begin{aligned} &amp;\min_a \ \   \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N a_ia_jy_iy_j(x_i \cdot x_j) - \sum_{i=1}^Na_i \\ &amp;s.t. \ \ \ \ \  0 \le a_i \le C , \ i = 1,2,…,N\\ &amp;  \ \ \ \ \ \ \ \ \ \sum_{i=1}^Na_iy_i = 0,\  i = 1,2,…,N \end{aligned}</p>
</li>
<li><p>采用 <strong>SMO</strong> 算法求解 $a_, ~i = 1,2,….N$；</p>
</li>
<li><p>根据 KKT 条件和 $a_i^*$ 解算出 $\mathbf{w}^*~,b^*,$。<br>\begin{aligned}<br>&amp;\mathbf{w}^* = \sum_{i=1}^Na_i^*y_ix_i\\ &amp;b^* = y_j - \sum_{i=1}^N y_ia_i^*(x_i \cdot x_j)  \\\<br>&amp;0 &lt; a^*_j &lt; C<br>\end{aligned}</p>
</li>
<li><p>得到超平面 $\mathbf{w}^* \cdot x + b^* = 0$。对于需要分类的数据，根据 $f(x) = sign(\mathbf{w}^* \cdot x+b^*)$ 判断其类别，其中 $sign$ 为相应的决策函数。</p>
</li>
</ol>
<hr>
<h1 id="5-非线性支持向量机"><a href="#5-非线性支持向量机" class="headerlink" title="5. 非线性支持向量机"></a>5. 非线性支持向量机</h1><p>对于给定的非线性可分数据集 $\left \{ (x_i,y_i)\right \}_{i=1}^N$，找不到一个分类平面将数据集分类。自然的想法就是将数据映射到新的空间 $\mathbf{x} \rightarrow \phi(\mathbf{x})$，使得其在新的空间中存在分类平面 $\mathbf{w}^T \cdot \phi(\mathbf{x}) +b= 0$，将其分类。如图 3 所示。<br><img src="/img/svm/svm3.png" title="图3. 左边为非线性可分，投影之后线性可分."></p>
<p>通常来说，非线性 SVM 不显式地定义映射函数 $\phi(\mathbf{x})$，而是采用核技巧，原因下面讲。</p>
<h2 id="5-1-核技巧"><a href="#5-1-核技巧" class="headerlink" title="5.1. 核技巧"></a>5.1. 核技巧</h2><p>针对非线性可分数据集，假设找到了映射函数 $\phi(\mathbf{x})$，使其线性可分。与线性支持向量机推导一样，约束最优化问题为：</p>
<p>\begin{equation}<br>\begin{aligned} &amp;\min_a \ \   \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N a_ia_jy_iy_j\color{red}{(\phi{(x_i)}^T \cdot \phi{(x_j)})}- \sum_{i=1}^Na_i \\ &amp;s.t. \ \ \ \ \  0 \le a_i \le C , \ i = 1,2,…,N\\ &amp;  \ \ \ \ \ \ \ \ \ \sum_{i=1}^Na_iy_i = 0,\  i = 1,2,…,N \end{aligned}<br>\end{equation}<br>可以看到，上式的优化问题需要计算 $K(x_i,x_j)= \phi{(x_i)}^T \cdot \phi{(x_j)}$，我们称 $K(x_i,x_j)$ 为核函数。因为特征空间通常很高维，甚至是无穷维，映射函数的内积 $\phi{(x_i)}^T \cdot \phi{(x_j)}$ 并不容易计算。为了避开这个障碍，特征空间的內积用核函数 $K(,)$ 来代替，从而避免高维中的內积。所以，不需要定义映射函数 $\phi{\mathbf{x}}$，只需要定义核函数，这便是核技巧。核技巧的好处在于不需要显式定义映射函数，只需要选择合适的核函数。此外，值得说明的是，正是对偶问题的引入，才能使得应用核技巧。</p>
<h2 id="5-2-核函数"><a href="#5-2-核函数" class="headerlink" title="5.2. 核函数"></a>5.2. 核函数</h2><p>显然，若已知映射函数，则可写出核函数。但是现实任务中我们通常不知道映射函数是什么样的形式。那么，什么样的函数 $K(,)$ 可以作为一个有效的核函数呢？答案是只要 $K(,)$ 满足 <strong>Mercer</strong> 定理即可。这里不再展开叙述。</p>
<p>定义了核函数，实际上就定义了一个新的特征空间，这个新的特征空间称之为再生核希尔伯特空间。需要注意的是，在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，而核函数也仅是隐式的定义了新的特征空间。于是，核函数的选用成为了非线性支持向量机的最大变数。若核函数选择不合适，则意味着数据映射到了一个不合适的特征空间，很可能会导致分类效果不佳。</p>
<p>显然，判断一个函数是否可以当成核函数或者判断一个核函数是否适合是很困难的。这里给出一些常用的核函数。</p>
<ol>
<li><p>线性核<br>\begin{equation*}<br>K(x_i,x_j) = x_i^T~x_j +c<br>\end{equation*}</p>
</li>
<li><p>多项式核<br>\begin{equation*}<br>K(x_i,x_j) = (x_i^T~x_j +c)^d<br>\end{equation*}</p>
</li>
<li><p>高斯核<br>\begin{equation*}<br>K(x_i,x_j) = exp(\frac{-||x_i-x_j||^2}{2\sigma^2})<br>\end{equation*}</p>
</li>
<li><p>拉普拉斯核<br>\begin{equation*}<br>K(x_i,x_j) = exp(\frac{-||x_i-x_j||}{\sigma})<br>\end{equation*}<br>此外，也可通过函数组合得到。</p>
</li>
</ol>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>[1] 《<a href="https://book.douban.com/subject/26708119/" target="_blank" rel="external">机器学习</a>》, 周志华</p>
<p>[2] 《<a href="https://book.douban.com/subject/10590856/" target="_blank" rel="external">统计学习方法</a>》, 李航</p>
<p>[3] 《<a href="http://web.stanford.edu/~boyd/cvxbook/" target="_blank" rel="external">Convex Optimization</a>》, Stephen Boyd and Lieven Vandenberghe</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leeliang.github.io/2017/08/24/nBayes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang LI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/24/nBayes/" itemprop="url">机器学习算法(2): 朴素贝叶斯分类器</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-24T20:20:25+08:00">
                2017-08-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1-贝叶斯决策论"><a href="#1-贝叶斯决策论" class="headerlink" title="1. 贝叶斯决策论"></a>1. 贝叶斯决策论</h1><p>考虑一个简单的二分类问题，假设有一个输入向量 $x$，我们的目标是对于 $x$ 预测它的类别 $y$，其中 $y = \{C_1, C_2\}$。现在我们有一个算法对 $x$ 进行了分类，这个算法会把输入空间划分为不同的区域 $R_k$，区域 $R_k$ 中的所有点都被分类为 $C_k$。假设现在我们有个输入，根据算法它被划分在区域 $R_1$，即我们的决策结果为 $C_1$，但实际上它是 $C_2$ 类，这样我们就犯了分类错误，反之亦然。在决策过程中，我们希望能最小化错误分类率 $p(mistake)$：<br>\begin{equation}<br>\begin{split}<br>p(mistake) &amp;= p( x \in R_1,C_2) + p( x \in R_2,C_1)<br>\\\<br>&amp;= \int_{R_1} p(C_2,x) ~dx +\int_{R_2} p(C_1,x) ~dx<br>\end{split}<br>\end{equation}<br>即 $R_1$ 区域中，实际类别为 $C_2$ 的样本尽量少；$R_2$ 区域中，实际类别为 $C_1$ 的样本尽量少。</p>
<p>实际情况中，我们面对的情况要比最小化错误率复杂，因为把 $C_1$ 类错分了 $C_2$ 类和把 $C_2$ 类错分为 $C_1$ 类的代价是不一样。我们需要对不同的错误加一个权值，这就是期望损失 (expected loss)：<br>\begin{equation}<br>EL = \int_{R_1} \color{red}{\lambda_{21}}~ p(C_2,x) ~dx +\int_{R_2}\color{red}{\lambda_{12}}~ p(C_1,x) ~dx<br>\end{equation}<br>其中，$\lambda_{21}$ 是将一个真实标记为 $C_1$ 误分类为 $C_2$ 的损失，反之同理。</p>
<p>更一般地，对于 $K$ 类的问题，期望损失为：<br>\begin{equation}<br>EL =  \sum_{k} \sum_{j} \int_{R_j} \lambda_{kj}~~p( C_k, x)~ dx<br>\end{equation}</p>
<p>对于每一个样本 $x$，我们要将其划分到决策区域 $R$ 中，那么划分到 $R_1$ 还是 $R_2$ ……中呢？我们希望期望损失能够最小化。也就是说，对于每个 $x$，我们将其划分到 $R_j$ 中，这样的结果下其期望损失最小：<br>\begin{equation}<br>arg~ \min ~\sum_k~\lambda_{kj}~p( C_k, x)<br>\end{equation}</p>
<p>将联合概率形式写成条件概率：<br>\begin{equation}<br> arg~ \min ~\sum_k~\lambda_{kj}~p(C_k \mid x)~p(x)<br> \label{5}<br>\end{equation}</p>
<p>注意到 $p(x)$ 对于所有项都相同，最小化 (\ref{5}) 式等价于：<br>\begin{equation}<br> arg~ \min ~\sum_k~\lambda_{kj}~p(C_k \mid x)<br>  \label{6}<br>\end{equation}</p>
<h1 id="2-后验概率最大化"><a href="#2-后验概率最大化" class="headerlink" title="2. 后验概率最大化"></a>2. 后验概率最大化</h1><p>若我们的损失函数简化为：<br>\begin{equation}<br>\lambda_{kj} = \begin{cases}1,&amp;k≠j\\0,&amp;k=j\end{cases}<br>\end{equation}<br>(\ref{6}) 式等价于：<br>\begin{equation}<br>\begin{split}<br>f(x) &amp;= arg~ \min ~\sum_k~\lambda_{kj}~p(C_k \mid x) \\\<br>&amp; = arg~ \min ~\sum_{k\neq j}~ p(C_k \mid x)<br>\\\<br>&amp; = arg~ \min ~[1- p(C_{k = j} \mid x)]<br>\\\<br>&amp; = arg \max p(C_j \mid x)<br>\end{split}<br>\end{equation}<br>对于每个 $x$，我们将其划分到 $R_j$ 中，最大化 $p(C_j \mid x)$ 就能够使得期望损失最小化。也就是说，后验概率最大等价于 $0-1$ 损失函数时的期望损失最小。记决策结果为 $c= R_j$，我们需要最大化 $p(c \mid x)$。在现实任务中，$p(c \mid x)$ 通常难以直接获取，需要使用贝叶斯公式。</p>
<p>基于贝叶斯定理，有：<br>\begin{equation}<br>p(c \mid x) = \frac{p(c) ~p(x \mid c)}{p(x)}<br>\end{equation}<br>其中 $p(c \mid x)$ 称之为后验概率。</p>
<h1 id="3-朴素贝叶斯分类器"><a href="#3-朴素贝叶斯分类器" class="headerlink" title="3. 朴素贝叶斯分类器"></a>3. 朴素贝叶斯分类器</h1><p>为了得到我们的模型，需要最大化后验概率，因为：<br>\begin{equation} p(c \mid x) \propto p(c) ~p(x \mid c) \end{equation}<br>最大化后验概率等价于：<br>\begin{equation}<br>\label{11}<br>arg \max p(c) ~p(x \mid c) \end{equation}<br>也就是需要估计先验概率 $p(c)$ 和类条件概率 $p(x \mid c)$。针对具体问题，类条件概率 $p(x \mid c)$ 是所有特征 $x$ 上的联合概率，难以从有限的样本直接估计而得。为避开这个问题，朴素贝叶斯分类器采用了特征独立性假设：即对已知类别，假设所有属性相互独立。基于该假设，(\ref{11}) 式重写为：<br>\begin{equation}<br>arg \max p(c) ~\prod_{i=1}^d p(x_i \mid c)<br>\end{equation}</p>
<p>因此，朴素贝叶斯分类器的训练过程就是基于数据集 <code>D</code> 来估算先验概率 $p(c)$ 和每个属性的条件概率 $p(x_i \mid c)$。</p>
<p>假设训练过程已经完成，有一个新的输入 $x=\{x_1, x_2, x_3\cdots\}$，需要采用朴素贝叶斯来进行二分类 $y = \{0,1\}$，其过程为：</p>
<ul>
<li>根据训练的结果：先验概率和每个属性的条件概率，计算 $p(0) ~\prod\limits_{i=1}^d p(x_i \mid 0)$；</li>
<li>根据训练的结果：先验概率和每个属性的条件概率，计算 $p(1) ~\prod\limits_{i=1}^d p(x_i \mid 1)$；</li>
<li>那个大划分为那类。</li>
</ul>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>[1] 《<a href="https://book.douban.com/subject/26708119/" target="_blank" rel="external">机器学习</a>》, 周志华</p>
<p>[2] Pattern Recognition and Machine Learning, Christopher M. Bishop.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leeliang.github.io/2017/08/21/decision-tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang LI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/21/decision-tree/" itemprop="url">机器学习算法(1): 决策树</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-21T20:20:25+08:00">
                2017-08-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1-决策树概述"><a href="#1-决策树概述" class="headerlink" title="1. 决策树概述"></a>1. 决策树概述</h1><p>决策树是一种基本的分类和回归算法。其根据数据的特征，采用树结构进行决策的方法。通常包括 3 个步骤：</p>
<ul>
<li>特征划分选择</li>
<li>决策树生成</li>
<li>剪枝处理</li>
</ul>
<p>一般地，一颗决策树包含一个根节点，若干内部节点和若干叶节点。其算法的基本流程如下，其中 <code>D</code> 为数据集，<code>A</code> 为特征集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">creatTree</span><span class="params">(D, A)</span>:</span></div><div class="line">	根据 D 生成叶节点集 node  <span class="comment"># 数据标记</span></div><div class="line">	<span class="keyword">if</span> len(node)==<span class="number">1</span>:       <span class="comment"># D 中样本属于同一类别</span></div><div class="line">		标记所有数据为同一叶节点</div><div class="line">	<span class="keyword">if</span> A == [] <span class="keyword">or</span> len(D.feature)==<span class="number">1</span> <span class="comment"># D 中只有一个特征</span></div><div class="line">		将 node 标记为叶节点，其 label 为 D 中样本数量最多的类</div><div class="line">	<span class="comment">#</span></div><div class="line">	从 A 中挑出最优特征 a</div><div class="line">	<span class="comment">#</span></div><div class="line">	<span class="keyword">for</span> value <span class="keyword">in</span> a:</div><div class="line">		生成新的特征集 A* = (A-a)</div><div class="line">		生成新的数据集 D* 表示 D 中取值为 value 的样本子集</div><div class="line">		<span class="keyword">if</span> D* == []</div><div class="line">			将 node 标记为叶节点，其 label 为 D* 中样本数量最多的类</div><div class="line">		<span class="keyword">else</span>:</div><div class="line">			creatTree(D*, A*)</div><div class="line">	<span class="keyword">return</span> tree</div></pre></td></tr></table></figure>
<p>可以看到，算法第 8 行是决策树算法最关键的内容，即特征划分的选择：怎么选出最优特征。</p>
<h1 id="2-特征划分选择"><a href="#2-特征划分选择" class="headerlink" title="2. 特征划分选择"></a>2. 特征划分选择</h1><p>我们希望采用某个值，按照该值可以对 A 中的特征进行排序，自然就选出了最优的特征。</p>
<h2 id="2-1-信息增益"><a href="#2-1-信息增益" class="headerlink" title="2.1. 信息增益"></a>2.1. 信息增益</h2><p>假定当前数据集 D 中第 $k$ 类样本所占比例为 $p_k$，则 D 的信息熵为：<br>\begin{equation}<br>entropy(D) = -\sum_{k=1}^n p_k log_2 p_k<br>\end{equation}<br>假设属性 $a$ 有 $V$ 个取值，记 $D^v$ 为取值为 $a^v$ 的样本子集。用属性 $a$ 划分数据集所获得的信息增益为：<br>\begin{equation}<br>Gain(D,a) = entropy(D) - \sum^V_1 \frac{|D^v|}{D}entropy(D^v)<br>\label{id3}<br>\end{equation}<br>信息熵代表信息的混沌程度，信息增益的含义为：数据的混沌程度减去用  $a$ 划分后的数据混沌程度，划分后的数据集混沌程度越小，信息增益越大。因此我们可以用信息增益作为准则来选择最优特征。</p>
<h2 id="2-2-ID3"><a href="#2-2-ID3" class="headerlink" title="2.2. ID3"></a>2.2. ID3</h2><p>ID3 就是采用公式 (\ref{id3}) 作为准则来划分特征的。</p>
<h2 id="2-3-C4-5"><a href="#2-3-C4-5" class="headerlink" title="2.3. C4.5"></a>2.3. C4.5</h2><p>信息增益偏好于取值数目较多的特征，为了减少这种偏好，C4.5 采用增益率作为划分特征的准则：<br>\begin{equation}<br>Gain-Ratio(D,a) = \frac{Gain(D,a)}{IV(a)}<br>\label{c4.5}<br>\end{equation}<br>其中：<br>\begin{equation}<br>IV(a) = -\sum_1^V \frac{|D^v|}{D} log_2 \frac{|D^v|}{D}<br>\end{equation}<br>与 $a$ 的取值数目成正比，因此，增益率准则偏好于取值数目较少的特征，为了减少这种偏好，C4.5 并不是直接采用增益率作为划分准则，而是分为两步：</p>
<ol>
<li>找出信息增益高于平均水平的属性；</li>
<li>从上述属性集中选取增益率最高的属性。</li>
</ol>
<h2 id="2-4-CART"><a href="#2-4-CART" class="headerlink" title="2.4 CART"></a>2.4 CART</h2><p>CART 采用基尼指数作为划分准则。基尼指数定义为：<br>\begin{equation}<br>Gini-Index(D,a) = \sum_1^V \frac{|D^v|}{D} Gini(D^v)<br>\end{equation}<br>其中，基尼值定义为：<br>\begin{equation}<br>Gini(D) = 1 - \sum_1^n p^2_k<br>\end{equation}<br>表示从数据集 D 中随机抽取两个不一样的样本的概率。</p>
<h1 id="3-剪枝处理"><a href="#3-剪枝处理" class="headerlink" title="3. 剪枝处理"></a>3. 剪枝处理</h1><p>剪枝是为了处理过拟合，基本策略有：</p>
<ul>
<li>预剪枝：在决策树生成过程中，对每个节点划分前先进行估计，若不能带来泛化性能提升，则停止划分；</li>
<li>后剪枝：先生成一颗完整的树，自下而上的对非叶节点进行估计，若该节点对应的子树替换成叶节点能带来泛化性能提升，则将子树替换为叶节点。</li>
</ul>
<p>预剪枝有可能引起欠拟合，后剪枝通常比预剪枝保留更多分支。一般来说，后剪枝剪枝效果好但是时间花销大。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>[1] 《<a href="https://book.douban.com/subject/26708119/" target="_blank" rel="external">机器学习</a>》, 周志华</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leeliang.github.io/2017/08/14/mcmc3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang LI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/14/mcmc3/" itemprop="url">MCMC (3): MCMC采样方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-14T15:20:06+08:00">
                2017-08-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Math/" itemprop="url" rel="index">
                    <span itemprop="name">Math</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1-MCMC采样算法"><a href="#1-MCMC采样算法" class="headerlink" title="1. MCMC采样算法"></a>1. MCMC采样算法</h1><p>在上一篇文章 <a href="/2017/08/10/mcmc2/" title="《MCMC (2): 马尔科夫链》">《MCMC (2): 马尔科夫链》</a> 中，我们提到需要构造一个转移矩阵 $Q(q(i,j))$，使其满足细致平稳条件：<br>\begin{equation}<br>p(i)q(i,j) = p(j)q(j,i)<br>\label{dbc}<br>\end{equation}</p>
<p>假设我们已经有一个转移矩阵为 $Q$ 的马氏链，$q(i,j)$ 表示从状态 $i$ 转移到 $j$ 的概率，通常情况下，我们随便取得转移矩阵不满足细致平稳条件，即：<br>\begin{equation}<br>p(i) q(i,j) \neq p(j) q(j,i)<br>\end{equation}<br>也就是说，这个转移矩阵不是所需要的。我们通过对其改造一下，使其满足细致平稳条件，如引入一个参数 $\alpha(i,j)$，使得：<br>\begin{equation}<br>p(i) q(i,j)\alpha(i,j) = p(j) q(j,i)\alpha(j,i)<br>\label{equ3}<br>\end{equation}<br>按照对称性，我们有：<br>\begin{equation}<br>\alpha(i,j)= p(j) q(j,i)， \quad \alpha(j,i) = p(i) q(i,j)<br>\end{equation}<br>于是，我们得到了满足细致平稳条件的转移矩阵 $Q’$:<br>\begin{equation}<br>Q’(i,j) = q(i,j)\alpha(i,j)<br>\end{equation}<br>在改造 $Q’$ 过程中引入的 $\alpha(i,j)$ 成为接受率，可以理解为从状态 $i$ 以 $q(i,j)$ 的概率跳转到状态 $j$ 的时候，我们以 $\alpha(i,j)$ 的概率接受这个转移，于是得到了新的马氏链的转移概率为 $Q’(i,j)=q(i,j)\alpha(i,j)$。</p>
<h2 id="1-1-Metropolis采样方法"><a href="#1-1-Metropolis采样方法" class="headerlink" title="1.1. Metropolis采样方法"></a>1.1. Metropolis采样方法</h2><p>假设我们已经有一个转移矩阵 $Q$，将上述的理论实现为方法，具体过程为：</p>
<ol>
<li>初始化马氏链状态，采样 $x_0$，其中 $x_0$ 服从 $\pi_0(x)$ 分布；</li>
<li>以 $x_0$ 为中心，根据转移矩阵 $q(x_1 \mid X=x_0)$ 采样  $x_1$，其中 $x_1$ 服从 $\pi_1(x)=\pi_0(x) Q$ 分布；</li>
<li>考虑到我们的转移矩阵不满足细致平稳条件，需要按照接受率 $\alpha(i,j)$ 确定是否接受从 $0$ 状态转移到 $1$ 状态；</li>
<li>若不接受，重新以 $x_0$ 为中心采样 $x_1$;</li>
<li>若接受，以 $x_1$ 为中心采样 $x_2$，其中 $x_2$ 服从 $\pi_2(x)=\pi_1(x) Q’$</li>
<li>重复以上过程，根据马氏链收敛定理，概率分布将收敛到 $\pi(x)$，即采样的 $x_n, x_{n+1}, x_{n+2}\cdots$将都服从 $\pi(x)$ 分布，而 $\pi(x)$ 正是我们需要采样的概率分布。这样，我们就得到了概率分布 $\pi(x)$ 的样本。</li>
</ol>
<p>具体算法如下：<br><img src="/img/mcmc/metropolis.jpg" title="图1. 算法示意"></p>
<h2 id="1-2-MH-采样算法"><a href="#1-2-MH-采样算法" class="headerlink" title="1.2. MH 采样算法"></a>1.2. MH 采样算法</h2><p>上述的采样算法存在一个小问题：在马氏链的转移过程中，接受率可能偏小，这样采样过程容易原地踏步，收敛到平稳分布的速度过慢。为了扩大接受率，我们将 (\ref{equ3}) 式两边同时乘上一个系数，同比例的放大 $\alpha(i,j)$ 和 $\alpha(j,i)$, 使得两个数中最大的为 1，这样在满足细致平稳条件的情况下提高了接受率，这就是 Metropolis-Hastings 采样算法。<br>具体过程如下：<br><img src="/img/mcmc/mh.jpg" title="图2. 算法示意"></p>
<h1 id="2-MH算法-PYTHON-实现"><a href="#2-MH算法-PYTHON-实现" class="headerlink" title="2. MH算法 PYTHON 实现"></a>2. MH算法 PYTHON 实现</h1><p>我们还是采用 <a href="/2017/08/10/mcmc2/" title="《MCMC (2): 马尔科夫链》">《MCMC (2): 马尔科夫链》</a> 中的例子，采样 $p = [0.9, 0.05, 0.05]$ 的分布的样本，即下层收入的人占比 90%，中层占比 5%，上层占比 5%。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line">p=np.array([<span class="number">0.9</span>,<span class="number">0.05</span>,<span class="number">0.05</span>])</div><div class="line"><span class="comment"># 初始化转移矩阵 </span></div><div class="line">A = np.array([p <span class="keyword">for</span> x <span class="keyword">in</span> range(len(p))], dtype=np.float32)</div><div class="line"><span class="comment"># 采样计数 </span></div><div class="line">samplecount = <span class="number">0</span></div><div class="line">sMax = <span class="number">100000</span></div><div class="line">samples = []</div><div class="line"><span class="comment"># 马氏链初始状态</span></div><div class="line">i = np.random.randint(len(p))</div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="comment"># 以状态 i 为中心，根据转移矩阵采样状态 j</span></div><div class="line">    j = np.argmax(np.random.multinomial(<span class="number">1</span>, A[i]))</div><div class="line">    <span class="comment"># 计算接受率 (A[j][i]*p[j]) / (A[i][j]*p[i])</span></div><div class="line">    alpha = min(<span class="number">1</span>,(A[j][i]*p[j])/(A[i][j]*p[i]))</div><div class="line">    <span class="comment"># 生成u</span></div><div class="line">    u = np.random.uniform()</div><div class="line">    <span class="keyword">if</span> u &lt; alpha:</div><div class="line">    	 <span class="comment"># 转移至状态 j </span></div><div class="line">        i = j</div><div class="line">        samplecount += <span class="number">1</span></div><div class="line">        samples.append(j)</div><div class="line">    <span class="comment"># if u &gt; alpha, 状态不转移，依旧从状态 i 采样</span></div><div class="line">    <span class="keyword">if</span> samplecount &gt;= sMax:</div><div class="line">        <span class="keyword">break</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">continue</span></div><div class="line">        </div><div class="line">samples = np.array(samples)</div><div class="line">sizes = [np.sum(samples==<span class="number">0</span>), np.sum(samples==<span class="number">1</span>), np.sum(samples==<span class="number">2</span>)]</div><div class="line">plt.figure(figsize=[<span class="number">3</span>,<span class="number">3</span>])</div><div class="line">plt.pie(sizes,autopct=<span class="string">'%1.1f%%'</span>, shadow=<span class="keyword">True</span>)</div></pre></td></tr></table></figure></p>
<p>结果如下：<br><img src="/img/mcmc/pie.png" title="图3. 采样结果"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leeliang.github.io/2017/08/10/mcmc2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang LI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/10/mcmc2/" itemprop="url">MCMC (2): 马尔科夫链</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-10T14:41:04+08:00">
                2017-08-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Math/" itemprop="url" rel="index">
                    <span itemprop="name">Math</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1-为什么需要马尔科夫链？"><a href="#1-为什么需要马尔科夫链？" class="headerlink" title="1. 为什么需要马尔科夫链？"></a>1. 为什么需要马尔科夫链？</h1><p>在上一篇文章 <a href="/2017/08/07/mcmc1/" title="《MCMC (1): 蒙特卡洛方法》">《MCMC (1): 蒙特卡洛方法》</a> 中，我们总结了蒙特卡洛方法解决问题的三个步骤为：</p>
<blockquote>
<ul>
<li>构造概率过程；</li>
<li>采样；</li>
<li>估计参数。</li>
</ul>
</blockquote>
<p><code>马尔科夫链也就是需要解决上述的采样问题。</code></p>
<p>在计算机中，均匀分布 $Uniform(0,1)$ 的样本相对容易生成，常见的概率分布也可以基于均匀分布生成。但是，当我们的概率分布很复杂或者是高维分布时，样本的生成就存在困难，这时候就需要借助马尔科夫链了。也就是说，马尔科夫链的性质可以帮助我们近似的生成符合某概率分布的样本。为什么呢？</p>
<h1 id="2-马尔科夫链"><a href="#2-马尔科夫链" class="headerlink" title="2. 马尔科夫链"></a>2. 马尔科夫链</h1><p>马尔科夫链的定义很简单：</p>
<p>\begin{equation}<br>P(X_{t+1} = x \mid X_t, X_{t-1}, \cdots) =P(X_{t+1}=x \mid X_t)<br>\end{equation}</p>
<p>也就是状态转移的概率只依赖于前一个状态。符合这样定义的马氏链有什么性质呢？让我们看一个例子。</p>
<p>社会学家经常把人按其经济状况分成 3 类：低层、中层、高层 ，我们用 1、2、3 分别代表这三个阶层。社会学家们发现决定一个人的收入阶层只与其父母的收入阶层有关，也就说收入状态转移的概率只依赖于前一个状态(马尔科夫链)。具体的转移概率如下图所示：<br><img src="/img/mcmc/uml.png" title="图1. 转移概率"></p>
<p>也就说如果一个人的父代的收入阶层是底层，他属于底层的概率是 0.65、属于中层的概率是 0.28、属于高层的概率是 0.07 (寒门难出贵子！)。将转移概率写成矩阵：</p>
<p>\begin{equation}<br>P =<br>\begin{bmatrix}<br>0.65 &amp; 0.28 &amp; 0.07 \\\<br>0.15 &amp; 0.67 &amp; 0.18 \\\<br>0.12 &amp; 0.36 &amp; 0.52 \\\<br>\end{bmatrix}<br>\end{equation}</p>
<p>假设某一代为初始状态，初始概率分布为 $\pi_0 = [0.21,0.68,0.11]$，则根据转移矩阵我们可以计算 $n$ 代人的收入阶层分布：</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">下层比例</th>
<th style="text-align:center">中层比例</th>
<th style="text-align:center">高层比例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">第0代人</td>
<td style="text-align:center">0.210</td>
<td style="text-align:center">0.680</td>
<td style="text-align:center">0.110</td>
</tr>
<tr>
<td style="text-align:center">第1代人</td>
<td style="text-align:center">0.252</td>
<td style="text-align:center">0.554</td>
<td style="text-align:center">0.194</td>
</tr>
<tr>
<td style="text-align:center">第2代人</td>
<td style="text-align:center">0.270</td>
<td style="text-align:center">0.512</td>
<td style="text-align:center">0.218</td>
</tr>
<tr>
<td style="text-align:center">第3代人</td>
<td style="text-align:center">0.278</td>
<td style="text-align:center">0.497</td>
<td style="text-align:center">0.225</td>
</tr>
<tr>
<td style="text-align:center">第4代人</td>
<td style="text-align:center">0.282</td>
<td style="text-align:center">0.492</td>
<td style="text-align:center">0.226</td>
</tr>
<tr>
<td style="text-align:center">第5代人</td>
<td style="text-align:center">0.284</td>
<td style="text-align:center">0.490</td>
<td style="text-align:center">0.226</td>
</tr>
<tr>
<td style="text-align:center">第6代人</td>
<td style="text-align:center">0.285</td>
<td style="text-align:center">0.489</td>
<td style="text-align:center">0.225</td>
</tr>
<tr>
<td style="text-align:center">第7代人</td>
<td style="text-align:center">0.286</td>
<td style="text-align:center">0.489</td>
<td style="text-align:center">0.225</td>
</tr>
<tr>
<td style="text-align:center">第8代人</td>
<td style="text-align:center">0.286</td>
<td style="text-align:center">0.489</td>
<td style="text-align:center">0.225</td>
</tr>
<tr>
<td style="text-align:center">第9代人</td>
<td style="text-align:center">0.286</td>
<td style="text-align:center">0.489</td>
<td style="text-align:center">0.225</td>
</tr>
<tr>
<td style="text-align:center">第10代人</td>
<td style="text-align:center">0.286</td>
<td style="text-align:center">0.489</td>
<td style="text-align:center">0.225</td>
</tr>
</tbody>
</table>
<p>我们发现从第7代人开始，这个分布就稳定不变了，这个是偶然的吗？</p>
<p>我们换一个初始概率分布 $\pi_0 = [0.75,0.15,0.1]$ 试试：</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">下层比例</th>
<th style="text-align:center">中层比例</th>
<th style="text-align:center">高层比例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">第0代人</td>
<td style="text-align:center">0.750</td>
<td style="text-align:center">0.150</td>
<td style="text-align:center">0.100</td>
</tr>
<tr>
<td style="text-align:center">第1代人</td>
<td style="text-align:center">0.522</td>
<td style="text-align:center">0.346</td>
<td style="text-align:center">0.132</td>
</tr>
<tr>
<td style="text-align:center">第2代人</td>
<td style="text-align:center">0.407</td>
<td style="text-align:center">0.426</td>
<td style="text-align:center">0.167</td>
</tr>
<tr>
<td style="text-align:center">第3代人</td>
<td style="text-align:center">0.349</td>
<td style="text-align:center">0.459</td>
<td style="text-align:center">0.192</td>
</tr>
<tr>
<td style="text-align:center">第4代人</td>
<td style="text-align:center">0.318</td>
<td style="text-align:center">0.475</td>
<td style="text-align:center">0.207</td>
</tr>
<tr>
<td style="text-align:center">第5代人</td>
<td style="text-align:center">0.303</td>
<td style="text-align:center">0.482</td>
<td style="text-align:center">0.215</td>
</tr>
<tr>
<td style="text-align:center">第6代人</td>
<td style="text-align:center">0.295</td>
<td style="text-align:center">0.485</td>
<td style="text-align:center">0.220</td>
</tr>
<tr>
<td style="text-align:center">第7代人</td>
<td style="text-align:center">0.291</td>
<td style="text-align:center">0.487</td>
<td style="text-align:center">0.222</td>
</tr>
<tr>
<td style="text-align:center">第8代人</td>
<td style="text-align:center">0.289</td>
<td style="text-align:center">0.488</td>
<td style="text-align:center">0.224</td>
</tr>
<tr>
<td style="text-align:center">第9代人</td>
<td style="text-align:center">0.288</td>
<td style="text-align:center">0.488</td>
<td style="text-align:center">0.224</td>
</tr>
<tr>
<td style="text-align:center">第10代人</td>
<td style="text-align:center">0.287</td>
<td style="text-align:center">0.488</td>
<td style="text-align:center">0.225</td>
</tr>
<tr>
<td style="text-align:center">第11代人</td>
<td style="text-align:center">0.287</td>
<td style="text-align:center">0.488</td>
<td style="text-align:center">0.225</td>
</tr>
<tr>
<td style="text-align:center">第12代人</td>
<td style="text-align:center">0.287</td>
<td style="text-align:center">0.488</td>
<td style="text-align:center">0.225</td>
</tr>
</tbody>
</table>
<p>从上面的两个表格可以发现，两次分布都收敛了，而且都收敛到 $\pi=[0.286, 0.489, 0.225]$ 这个分布。<strong>这就是马尔科夫链的收敛性质</strong>。</p>
<p>这里我们不过多的讲述关于马尔科夫链的定理和性质，需近一步了解的可以参考本文后列出的参考资料。我们需要知道的是，对于一个概率分布来说，如果马尔科夫链中的每一步都让这个概率分布保持不变，我们就说这个概率分布是马尔科夫链的平稳分布。用公式可如下表示：<br>\begin{equation}<br>\pi P = \pi, \quad \sum_{i=0}^{\infty} \pi_i = 1<br>\end{equation}<br>其中 $\pi$ 是马尔科夫链的概率分布，$P$ 是转移概率矩阵。即概率分布乘上转移矩阵还是它本身，也就是概率分布在每一步都保持不变。而且 $\pi$ 是方程 $\pi P = \pi$ 的唯一非负解。</p>
<p>回到我们的正题。我们需要利用马尔科夫链的性质来生成复杂概率分布 $p(x)$ 的样本。很自然的，我们的想法是产生一条马氏链，使得它的平稳分布就是想要的分布。即，构造符合条件的转移矩阵，使其满足：<br>\begin{equation}<br>p(x)Q = p(x)<br>\end{equation}<br>因为 $p(x)$ 是上式的唯一解，我们可以根据上式来生成 $p(x)$ 的样本。</p>
<p>那么，如何构造转移矩阵呢？</p>
<p>假设我们有一个转移矩阵 $Q$($q(i,j)$)，其满足如下条件：<br>\begin{equation}<br>p(i)q(i,j) = p(j)q(j,i)<br>\label{dbc}<br>\end{equation}<br>$i,j$ 是分布中的两个状态，如上面例子中的低层、中层(就是 $Q$ 矩阵中的下标)。根据 (\ref{dbc}) 式，我们有：<br>\begin{align}<br>&amp; \sum_{i=1}^\infty p(i)q(i,j) = \sum_{i=1}^\infty p(j)q(j,i)<br>= p(j) \sum_{i=1}^\infty q(j,i) = p(j) \\\<br>&amp; \Rightarrow p Q = p<br>\end{align}<br>可以看到，只要构造满足 (\ref{dbc}) 式的转移矩阵，我们就可以得到平稳分布 $p$，(\ref{dbc}) 式也就是马氏链的细致平稳条件。根据 (\ref{dbc}) 式，利用随机采样的方法，就可以得到分布 $p$ 的样本。随机采样的方法将在下一章记录。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>[1] <a href="https://cosx.org/2013/01/lda-math-mcmc-and-gibbs-sampling" target="_blank" rel="external">LDA-math-MCMC 和 Gibbs Sampling</a></p>
<p>[2] Information Theory, Inference, and Learning Algorithms, David J.C. MacKay. (Chap. 29).</p>
<p>[3] Pattern Recognition and Machine Learning, Christopher M. Bishop. (Chap. 11).</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leeliang.github.io/2017/08/07/mcmc1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang LI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/07/mcmc1/" itemprop="url">MCMC (1): 蒙特卡洛方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-07T14:37:33+08:00">
                2017-08-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Math/" itemprop="url" rel="index">
                    <span itemprop="name">Math</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1-蒙特卡洛方法"><a href="#1-蒙特卡洛方法" class="headerlink" title="1. 蒙特卡洛方法"></a>1. 蒙特卡洛方法</h1><p>蒙特卡洛是用“模拟”解决问题的方法。具体来说，需要解决的问题是估计<strong>参数</strong>，该参数是某一随机事件的<strong>某个统计值</strong>，通过“模拟”大量该随机事件的样本，根据样本估计待估参数。</p>
<p>举个例子帮助理解：</p>
<blockquote>
<p>怎么利用蒙特卡洛方法估计$pi$?</p>
</blockquote>
<p>如下图所示：<br><img src="/img/mcmc/circle.png" title="图1. 示意图"></p>
<p>图中圆和正方形的面积比为：<br>\begin{equation*}<br>\frac{\pi r^2}{(2r)^2}=\frac{\pi}{4}<br>\end{equation*}<br>也就是说，只需要知道圆和它的外切正方形的面积比就可以求得 $\pi$。假设有这样的一个随机事件：</p>
<blockquote>
<p>往正方形中撒入点，点落在正方形中的位置是均匀分布，随后计算有多少比例的点落入了圆中(根据距离圆心的距离判断)，这个比例就是 $\frac{\pi}{4}$。</p>
</blockquote>
<p>上述过程就是蒙特卡洛方法，有以下三方面的内容：</p>
<ol>
<li>随机事件：正方形中撒入点；</li>
<li>根据随机事件的概率分布(均匀分布)模拟数据，生成样本；</li>
<li>根据样本的某统计值(在圆内的比例)估计待估参数 $pi$。</li>
</ol>
<p>一般地，蒙特卡洛方法的三个步骤为：</p>
<ol>
<li>构造概率过程：确定随机事件的概率分布，该随机事件的某统计量刚好是待估参数；</li>
<li>采样：根据随机事件的概率分布采样；</li>
<li>估计参数：根据采样的样本估算参数。</li>
</ol>
<h1 id="2-PYTHON实现"><a href="#2-PYTHON实现" class="headerlink" title="2. PYTHON实现"></a>2. PYTHON实现</h1><p>利用蒙特卡洛法估计 $pi$，代码如下:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">n = <span class="number">1000</span></div><div class="line">x = <span class="number">2.0</span>*np.random.random(size=n)<span class="number">-1.0</span></div><div class="line">y = <span class="number">2.0</span>*np.random.random(size=n)<span class="number">-1.0</span></div><div class="line">index = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(n) <span class="keyword">if</span> (x[i]**<span class="number">2</span>+y[i]**<span class="number">2</span>) &lt;<span class="number">1</span>]</div><div class="line">count = len(index)</div><div class="line">pi = <span class="number">4.0</span>*count/n</div><div class="line">print(<span class="string">"PI=%f"</span> %pi)</div></pre></td></tr></table></figure></p>
<p>随机模拟了1000个点，所得结果为：PI=3.164。</p>
<img src="/img/mcmc/circle.png" title="图2. 示意图2">

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leeliang.github.io/2017/07/26/frequentism-and-bayesianism/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang LI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/07/26/frequentism-and-bayesianism/" itemprop="url">频率主义和贝叶斯主义---参数估计</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-26T13:17:03+08:00">
                2017-07-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Math/" itemprop="url" rel="index">
                    <span itemprop="name">Math</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1-频率主义与贝叶斯主义"><a href="#1-频率主义与贝叶斯主义" class="headerlink" title="1. 频率主义与贝叶斯主义"></a>1. 频率主义与贝叶斯主义</h1><p>频率主义认为<strong>_概率_</strong>与事件的<strong>频率</strong>有关，贝叶斯主义认为<strong>_概率_</strong>是对事件的认知程度的度量。举个例子说明一下，假设我们多次测量了一个棍子的长度，得到的观测数据为： <code>Data = [L1, L2]</code>，其中 <code>L1</code> 占比 30%，<code>L2</code> 占比 70%（_假设测量结果就两种情况_）。  </p>
<ul>
<li><p>频率主义认为：棍子的长度 (length) 是固定值，即参数是固定的。由于测量误差的存在，测量结果 <code>L1</code> 发生的频率为 30%，<code>L2</code> 发生的频率为 70%。根据频率主义者对概率的认知，可用如下公式表示：\begin{equation*}<br>P(Data=L1 \mid length)= 30\%  \\\<br>P(Data=L2 \mid length)= 70\%<br>\end{equation*}<br>其意义为：在 length 的条件下(在棍子长度为固定某值得条件下)，得到 <code>L1</code> 的概率为 30%，得到 <code>L2</code> 的概率为 70%。</p>
</li>
<li><p>贝叶斯主义认为：观测结果 (Data) 是固定的，是我们对事件的认知。根据观测结果，棍子长度为 <code>L1</code> 的概率为 30%，为 <code>L2</code> 的概率为7 0%，即参数 (length) 不是固定的，而是一个分布。概率就是我们对棍子长度的认知程度，我们 30% 地认为棍子长度为 <code>L1</code>, 70% 地认为棍子长度为 <code>L2</code>，可用如下公式表示：<br>\begin{equation*}<br>P(length=L1 \mid Data)= 30\%  \\\<br>P(length=L2 \mid Data)= 70\%<br>\end{equation*}<br>其意思为：在观测数据 (Data) 的条件下，棍子长度为 <code>L1</code> 的概率为 30%，为 <code>L2</code> 的概率为 70%。</p>
</li>
</ul>
<p>为了更直观的说明频率主义和贝叶斯主义，根据上述例子，有如下图像。<br><img src="/img/frequentism/vs.png" title="图1. 频率主义和贝叶斯主义对比"></p>
<p>$\quad$</p>
<h1 id="2-频率主义参数估计"><a href="#2-频率主义参数估计" class="headerlink" title="2. 频率主义参数估计"></a>2. 频率主义参数估计</h1><p>我们用线性拟合作为例子，考虑下面的数据集合 x、y和y 的误差 e，数据来源于<a href="http://jakevdp.github.io/blog/2014/06/06/frequentism-and-bayesianism-2-when-results-differ/" target="_blank" rel="external">Frequentism and Bayesianism II: When Results Differ</a>。</p>
<p>先看一下数据长什么样。<br><img src="/img/frequentism/xye.png" width="400" height="320" title="图2. 用于拟合的数据"><br>我们的任务是找出一条直线来拟合数据，这里我们采用最大似然估计法(常用的最小二乘估计为最大似然估计的一种特例：正态最大似然估计)。</p>
<p>我们构造一个线性模型，参数有斜率和截距，模型定义如下：<br>\begin{equation}<br>\label{model}<br>\hat{y} = a_0x+a_1<br>\end{equation}<br>根据观测数据 x，y 集合，估计 $a=(a_0,a_1)$。<br>对于某一次测量值 ($d_i=(x_i,y_i,e_i)$)，假设测量误差按正态分布，该事件发生的概率分布满足：<br>\begin{equation}<br>\label{pro}<br>P(d_i \mid a) = \frac{1}{\sqrt{2\pi e_i^2}}exp[-\frac{(y_i-\hat{y}_i)^2}{2e_i^2}]<br>\end{equation}</p>
<p>似然函数为每个观测值概率的乘积：<br>\begin{equation}<br>\label{like}<br>\ell (d \mid a) = \prod_{i=1}^N P(d_i \mid a)<br>\end{equation}</p>
<p>最大似然法就是使似然函数最大，这里很好理解。还是使用上面棍子的例子，假设我们测得的数据为 (10.1 米, 9.9 米)，根据测量数据我们估计棍子到底多长。那怎么估计呢？考虑这样的一个问题：棍子长度为多少时，我们最大概率上得到的测量数据为 (10.1 米, 9.9 米)，答案呼之欲出，我们猜测棍子的长度为 10.0 米，因为这样我们最大概率获得了我们的观测数据。如果棍子长度为 1 米，我们基本上不可能得到(10.1 米, 9.9 米)这样的观测数据。也就是说，我们需要求一个 length，使得 $P(10.1 \mid length)\cdot P(9.9 \mid length)$ 最大，也就是最大似然法。</p>
<p>回到上面的线性拟合问题，我们需要估计参数 $a$，使得 $d$ 发生的概率最大。一般情况下，因为似然估计值可能非常小，通常更方便的做法是取似然函数的对数，将公式 (\ref{pro}) 代入公式 (\ref{like}) 并取对数：<br>\begin{equation}<br>\begin{split}<br>log \ell (d \mid a) = -\frac{1}{2}\sum_{i=1}^N[log(2\pi e_i^2)+ \frac{(y_i-\hat{y}_i)^2}{e_i^2}]<br>\end{split}<br>\end{equation}</p>
<p>\begin{equation}<br>log \ell (d \mid a) = const - \sum_{i=1}^N\frac{1}{2e_i^2}(y_i-\hat{y}_i^2)<br>\end{equation}</p>
<p>我们需要求解 $a$ 使得上式最大化，等价于最小化下式:<br>\begin{equation}<br>loss = \sum_{i=1}^N\frac{1}{2e_i^2}(y_i-\hat{y})<br>\label{loss}<br>\end{equation}</p>
<p>上式与我们的直觉很符合，为了找到一个最佳拟合直线，我们需要观测值与模型值的差的平方和最小，这也就是最小二乘方法，或者说最小平方损失。</p>
<p>公式 (\ref{loss}) 的解算方法很多，这里用矩阵的解算方法。记误差 $e^2$ 组成的矩阵为 $P$，$v= y_i-\hat{y}$ 组成的矩阵为 $V$，公式 (\ref{loss}) 最小化等价于：<br>\begin{equation}<br>V^T P V = min<br>\label{vpv}<br>\end{equation}</p>
<p>将 $v= y_i-\hat{y}$ 写成矩阵形式：<br>\begin{equation}<br>V = Ba -l<br>\end{equation}</p>
<p>其中，$a=[a_0, a_1]$，$l=[-y]$，$B=[-x, -1]$。</p>
<p>求解公式 (\ref{vpv}):<br>\begin{equation}<br>\begin{split}<br>&amp;\frac{\partial{V^TPV}}{\partial a}=2V^T P \frac{\partial V}{\partial{a}}= V^TPB=0 \\\<br>&amp;\Rightarrow B^TPV=0 \\\<br>&amp;\Rightarrow B^TP(Ba-l)=0 \\\<br>&amp;\Rightarrow a = (B^TPB)^{-1}B^TPl<br>\end{split}<br>\end{equation}</p>
<p>根据上式，即可利用频率主义的方法估计参数 $a$。所得结果如下图所示：<br><img src="/img/frequentism/mlf.png" width="400" height="320" title="图3 频率主义拟合结果"></p>
<hr>
<p><strong>总结</strong>：频率主义认为参数是固定的(斜率和截距)，利用最大似然法估计参数，使得在此参数条件下，得到该组观测数据的概率最大。  </p>
<hr>
<h1 id="3-贝叶斯主义参数估计"><a href="#3-贝叶斯主义参数估计" class="headerlink" title="3. 贝叶斯主义参数估计"></a>3. 贝叶斯主义参数估计</h1><p>贝叶斯主义认为测量数据是固定的，根据测量数据可以得到我们对参数的认知程度 (参数的概率分布)，得到了参数的概率分布也就求解了参数。</p>
<p>同样采用上述线性拟合的例子，需要估计的参数的概率分布可如下表示：<br>\begin{equation}<br>P(a \mid d)<br>\end{equation}</p>
<p>即在观测数据 (d) 的条件下，参数 (a) 的概率分布是什么？怎么样才能得到参数的分布 ($P(a \mid d)$)？</p>
<p>这里需要引入贝叶斯定理，贝叶斯定理是贝叶斯估计的理论基础，可如下表示：<br>\begin{equation}<br>P(a \mid d) = \frac{P(a) P(d \mid a)}{P(d)}<br>\label{beyes}<br>\end{equation}</p>
<p>贝叶斯定理可以这样理解，$a$ 和 $d$ 同时发生的概率可如下表示：<br>\begin{equation}<br>P(a \cap d) = P(a) P(d \mid a) = P(d) P(a \mid d)<br>\end{equation}</p>
<p>即$a$和$d$同时发生的概率可表示为：发生 $a$ 的概率乘上 $a$ 发生的条件下 $b$ 发生的概率，反之亦然。通过 $a$ 和 $b$ 同时发生这一桥梁，就可以建立起 $P(d \mid a)$ 和 $P(a \mid d)$ 的联系。</p>
<p>根据公式 (\ref{beyes})，就可以达到我们的目的，求解 $P(a \mid d)$。但是，$P(a)$、$P(d \mid a)$、$P(d)$怎么知道呢？这里，我们先介绍这几个量的概念。</p>
<ul>
<li>$P(a \mid d)$：参数 $a$ 的后验概率，依据测量数据 $d$ 得到的结果，就是我们需要计算的量。</li>
<li>$P(a)$：参数 $a$ 的先验概率，在获取测量数据 $d$ 之前，我们对 $a$ 的认知。</li>
<li>$P(d \mid a)$：似然度，表示观测数据 $d$ 信息，形式上与似然函数 $\ell (d \mid a)$ 相同。</li>
<li>$P(d)$： 标准化常量，观测数据 $d$ 独立于待估参数 $a$，对于$a$ 而言，$P(d)$ 可以认为是常数。</li>
</ul>
<p>公式 (\ref{beyes}) 可以改写为：<br>\begin{equation}<br>P(a \mid d) \propto P(a) \ell(d \mid a)<br>\end{equation}</p>
<p>这里即包含了贝叶斯方法的基本思路：假定要估计的参数是服从一定分布的随机变量；根据待估参数的先验分布，并结合观测数据信息，应用贝叶斯定理求得待估参数的后验分布，并根据待估参数的后验分布得到待估参数的估计量。</p>
<p>这时候，唯一不清楚的是先验分布 $P(a)$。先验分布是人们对参数的主观认识，需要事先提供。如果有多个测量方法对同一事件进行了测量，我们可以采用其他方法的结果作为先验事实，给出先验分布；当先验事实不存在时，可以主观的选择先验分布，如常用的扁平先验：<br>\begin{equation}<br>P(a) \propto 1<br>\end{equation}</p>
<p>后验分布的计算往往是难以直接计算的，因此，贝叶斯方法往往需要采用随机模拟的方法来生成符合后验分布的样本，从而得到后验分布。这里采用贝叶斯估计中常用的抽样方法：MCMC。MCMC 的原理这里不详细叙述。根据后验分布的表达式，利用 MCMC 方法生成满足后验分布的样本，用这些样本来确定我们的待估参数。所得结果如下。</p>
<ul>
<li>$a_0$的分布：<img src="/img/frequentism/a0.png" title="图4 $a0$ 的分布">
</li>
</ul>
<ul>
<li><p>$a_1$的分布：</p>
<img src="/img/frequentism/a1.png" title="图5 $a1$ 的分布">
</li>
<li><p>结果：</p>
<img src="/img/frequentism/mlf2.png" title="图6 贝叶斯主义拟合结果">
</li>
</ul>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>[1] <a href="http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/" target="_blank" rel="external">Frequentism and Bayesianism 系列博文</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Liang LI" />
          <p class="site-author-name" itemprop="name">Liang LI</p>
           
              <p class="site-description motion-element" itemprop="description">A mind needs books.</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">11</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">11</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/leeliang" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/your-user-name" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                    
                      微博
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.zhihu.com/people/kiay-lee" target="_blank" title="知乎">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      知乎
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="mailto:liang4lee@gmail.com" target="_blank" title="E-Mail">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                    
                      E-Mail
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liang LI</span>

  
</div>


  <div class="powered-by">
    Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
  </div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">
    Theme &mdash;
    <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
      NexT.Pisces
    </a>
  </div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  








  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}},
        "HTML-CSS": {linebreaks: {automatic: true}},
        SVG: {linebreaks: {automatic: true}}
    });
    </script>

    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
